{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this problem set I worked with Eugene Leypunskiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import string\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.clustering import DistributedLDAModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2(a): Make vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack_file = open('/project/cmsc25025/stackexchange/20161215StatsPostsMerged.csv','r').readlines()\n",
    "\n",
    "spark = SparkSession.builder.master('local[56]')\\\n",
    "                            .appName('stack_exchange')\\\n",
    "                            .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "                            .config(\"spark.driver.memory\", \"20g\")\\\n",
    "                            .config(\"spark.executor.memory\", \"20g\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "\n",
    "stack_data = spark.sparkContext.parallelize(stack_file[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to make vocabulary\n",
    "1. First we split the data by comma to get the document ID\n",
    "2. The first entry after splitting comma is always the doc ID\n",
    "3. Substitute a blank space for anything that's not a space or lowercase alphabets\n",
    "4. Split the string up into words\n",
    "5. Make them unicodes\n",
    "6. Return the document ID and words in list\n",
    "7. Count number of words throughout the corpus\n",
    "8. Get rid of any words that have less than or equal to 3 characters\n",
    "9. Get rid of stopwords\n",
    "10. Remove words that have less than or equal to 35 counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    sForID = s.split(',') # Splitting data to get document ID\n",
    "    docID = sForID[0] # first index after split is the docID\n",
    "    s = re.sub('[^a-z\\ \\']+', \" \", s.lower().strip()) # substitute everything except letters + space with white space\n",
    "    s = s.split()\n",
    "    s = [i.decode('utf-8') for i in s]                # Make Unicode\n",
    "    return docID, s                          # Return doc ID + list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = stack_data.map(lambda x: clean_string(x))\\\n",
    "                       .map(lambda x: Counter(x[1]))\\\n",
    "                       .reduce(lambda a,b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100928\n",
      "100847\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Counter({k:v for k,v in vocabulary.items() if len(k) > 3}).most_common()\n",
    "print len(vocabulary)\n",
    "vocabulary= dict(vocabulary)\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for i in stopWords:\n",
    "    try:\n",
    "        vocabulary.pop(i.decode('utf-8'))\n",
    "    except KeyError:\n",
    "        continue\n",
    "print len(vocabulary)\n",
    "vocabulary = dict(vocabulary)\n",
    "vocabulary = Counter({k:v for k,v in vocabulary.items() if v > 35 }).most_common()\n",
    "vocabulary = OrderedDict(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2(b): Topic modelling with k=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parser for mapping each entry to a sequence of word-id/count pairs\n",
    "\n",
    "def map_wordid_count(x, masterVocab):\n",
    "    docId = x[0]\n",
    "    counterObj = x[1]\n",
    "    dumTupleList = []\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i, word in enumerate(masterVocab):\n",
    "        if word in counterObj:\n",
    "            keys.append(i)\n",
    "            values.append(counterObj[word])\n",
    "\n",
    "    return (docId, Vectors.sparse(len(masterVocab), keys, values))\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split Data into train and test\n",
    "stack_dataTrain, stack_dataTest = stack_data.randomSplit([9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map each document to (docID, [word-ID, cnt]) tuples\n",
    "parsedDataTrain = stack_dataTrain.map(lambda x: (clean_string(x)))\\\n",
    "                                 .map(lambda x: (int(\"{:.0f}\".format(float(x[0]))), Counter(x[1])))\\\n",
    "                                 .map(lambda x: map_wordid_count(x, vocabulary))\\\n",
    "                                 .map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train LDA Model\n",
    "parsedDF = parsedDataTrain.toDF([\"id\",\"features\"])\n",
    "lda = LDA(k=30, maxIter=50)\n",
    "model = lda.fit(parsedDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of words and cnts from vocabulary\n",
    "masterWords = []\n",
    "masterCnts = []\n",
    "for i in vocabulary:\n",
    "    masterWords.append(i), \n",
    "    masterCnts.append(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For all topics, get term indices and term weights \n",
    "termIndices = model.describeTopics()\\\n",
    "                   .select(\"termIndices\").rdd.map(list).map(lambda x:x[0]).collect()\n",
    "termWeights = model.describeTopics()\\\n",
    "                   .select(\"termWeights\").rdd.map(list).map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "city 0.0345725092478\n",
      "votes 0.030151135499\n",
      "brand 0.0261120433168\n",
      "cities 0.0254135944357\n",
      "vote 0.0236359871983\n",
      "segment 0.0158739462686\n",
      "share 0.0137525408588\n",
      "paradox 0.0126938750792\n",
      "voting 0.0115244139544\n",
      "segments 0.0107176662112\n",
      "Topic 1\n",
      "mediation 0.0365241799042\n",
      "path 0.0239449346215\n",
      "active 0.0173472982642\n",
      "moderator 0.0161888502735\n",
      "cohort 0.0161786926524\n",
      "indirect 0.0141197163471\n",
      "birds 0.0122684843608\n",
      "bird 0.0120367255903\n",
      "mediator 0.0116623944653\n",
      "moderation 0.0107626096867\n",
      "Topic 2\n",
      "factor 0.0250511960019\n",
      "scores 0.0182340685304\n",
      "score 0.0173455771644\n",
      "items 0.0169062587737\n",
      "analysis 0.0164066931385\n",
      "scale 0.0157126885984\n",
      "factors 0.0125168358228\n",
      "variables 0.0121173948325\n",
      "item 0.0104841692606\n",
      "correlation 0.00937553655196\n",
      "Topic 3\n",
      "frequent 0.0337521862314\n",
      "support 0.0171045977602\n",
      "closed 0.0129723244854\n",
      "apriori 0.0115075955782\n",
      "maximal 0.00928910988429\n",
      "ancillary 0.00888977020348\n",
      "association 0.00789309118505\n",
      "cophenetic 0.00751760721279\n",
      "batting 0.00712804456771\n",
      "yards 0.00672733401526\n",
      "Topic 4\n",
      "ratio 0.069366604506\n",
      "odds 0.0563229919866\n",
      "risk 0.0280221378632\n",
      "meta 0.0271344386566\n",
      "analysis 0.0245806542592\n",
      "ratios 0.0232662966164\n",
      "studies 0.017384217606\n",
      "hazard 0.0172237404788\n",
      "study 0.0166526881356\n",
      "outcome 0.00841353868886\n",
      "Topic 5\n",
      "data 0.0355445225547\n",
      "would 0.00954494279363\n",
      "like 0.00905785246315\n",
      "distance 0.00819382946423\n",
      "points 0.00781974934228\n",
      "using 0.00777327650864\n",
      "clustering 0.007109810606\n",
      "cluster 0.00668022967367\n",
      "want 0.00613093600544\n",
      "plot 0.00609688260968\n",
      "Topic 6\n",
      "distribution 0.0218745611102\n",
      "function 0.0144471638451\n",
      "matrix 0.011168505281\n",
      "random 0.0106730700829\n",
      "variance 0.00790225706949\n",
      "likelihood 0.00705637443082\n",
      "mean 0.00659020264295\n",
      "variables 0.00626159679451\n",
      "given 0.00619432474915\n",
      "normal 0.00585343363912\n",
      "Topic 7\n",
      "combinations 0.0177708197068\n",
      "modularity 0.0102415200481\n",
      "mcfadden's 0.00966835806516\n",
      "mcfadden 0.00922354143571\n",
      "pseudo 0.00848108409431\n",
      "loglikelihood 0.0053216842623\n",
      "vglm 0.0041015992742\n",
      "cragg 0.00390594894913\n",
      "orientation 0.00353764617362\n",
      "newman 0.00313228755691\n",
      "Topic 8\n",
      "particle 0.0374779003284\n",
      "stress 0.0323393186337\n",
      "crime 0.0263847161576\n",
      "particles 0.0221435382941\n",
      "wealth 0.0178303571172\n",
      "detector 0.0101787150665\n",
      "contract 0.00901839773856\n",
      "contracts 0.00806639370786\n",
      "inflation 0.00638687310474\n",
      "dollars 0.00625825798218\n",
      "Topic 9\n",
      "wind 0.028220115452\n",
      "transactions 0.0221004461665\n",
      "fuzzy 0.022094589136\n",
      "customer 0.0215335539309\n",
      "transaction 0.0177412932949\n",
      "speed 0.0177044645015\n",
      "influential 0.0154326126962\n",
      "brownian 0.0145594189835\n",
      "customers 0.0139747158871\n",
      "motion 0.0117027774483\n",
      "Topic 10\n",
      "model 0.0299408939986\n",
      "regression 0.0247439166976\n",
      "variables 0.015307180216\n",
      "variable 0.0151135062118\n",
      "data 0.0114430111796\n",
      "linear 0.0105797824381\n",
      "would 0.00972850600065\n",
      "values 0.00810186932844\n",
      "using 0.00776710062691\n",
      "models 0.00716807663187\n",
      "Topic 11\n",
      "bugs 0.0398098569372\n",
      "perplexity 0.0213936881867\n",
      "walker 0.00973378334465\n",
      "yule 0.00968241046082\n",
      "pert 0.00544824394293\n",
      "disability 0.00539812204503\n",
      "plsa 0.00357251209311\n",
      "deprivation 0.00222749486073\n",
      "test 0.0021741118715\n",
      "would 0.00216786343291\n",
      "Topic 12\n",
      "unit 0.0477175449478\n",
      "root 0.0461920111281\n",
      "causality 0.0440430298182\n",
      "test 0.0409419668245\n",
      "granger 0.0351680500309\n",
      "fuller 0.0149644997876\n",
      "dickey 0.0142543266584\n",
      "wave 0.0135435475536\n",
      "causal 0.0110996240943\n",
      "causes 0.0107211000504\n",
      "Topic 13\n",
      "cointegration 0.0408531931299\n",
      "stationary 0.0170340453913\n",
      "vecm 0.0169357418393\n",
      "lambda 0.0166196119115\n",
      "cointegrated 0.0160022193355\n",
      "correction 0.0129813035523\n",
      "test 0.0125447940555\n",
      "johansen 0.0102035702562\n",
      "integrated 0.0100930520163\n",
      "cointegrating 0.00787071900583\n",
      "Topic 14\n",
      "word 0.0412231174184\n",
      "words 0.0387810040074\n",
      "text 0.020594429234\n",
      "sequence 0.0133857472745\n",
      "document 0.0117632736667\n",
      "mutual 0.0107841276349\n",
      "sequences 0.0104899143664\n",
      "documents 0.0101405820332\n",
      "length 0.00856511026079\n",
      "corpus 0.00765283565094\n",
      "Topic 15\n",
      "time 0.0474918063445\n",
      "series 0.0291454617876\n",
      "data 0.0276488823328\n",
      "model 0.0232655650659\n",
      "would 0.009443689882\n",
      "year 0.00843681839607\n",
      "using 0.00815969213015\n",
      "forecast 0.00730433790911\n",
      "arima 0.00724487371897\n",
      "models 0.00620443894852\n",
      "Topic 16\n",
      "probability 0.0249171170299\n",
      "time 0.0128527150192\n",
      "number 0.00983053080656\n",
      "would 0.00954058313066\n",
      "data 0.00741955999747\n",
      "state 0.00665585825966\n",
      "probabilities 0.00597088725941\n",
      "first 0.00564479155808\n",
      "bayesian 0.0056234494123\n",
      "problem 0.00557418891571\n",
      "Topic 17\n",
      "garch 0.058283397822\n",
      "delta 0.0316832717047\n",
      "arma 0.0209464495375\n",
      "model 0.0192585107766\n",
      "conditional 0.016093017595\n",
      "volatility 0.0157901734049\n",
      "arch 0.0150188587616\n",
      "function 0.0126733442104\n",
      "sigmoid 0.00884432702604\n",
      "neuron 0.00820757837463\n",
      "Topic 18\n",
      "test 0.0285070842116\n",
      "sample 0.0227923962896\n",
      "distribution 0.0195101178917\n",
      "mean 0.0168825977201\n",
      "value 0.0134609241053\n",
      "standard 0.0113931271202\n",
      "data 0.0113538489828\n",
      "confidence 0.00888727972193\n",
      "would 0.00884254640735\n",
      "normal 0.00875528314195\n",
      "Topic 19\n",
      "sampling 0.0578167565029\n",
      "species 0.0343473456336\n",
      "mcmc 0.0277832703231\n",
      "carlo 0.0264401238778\n",
      "monte 0.0264398465407\n",
      "gibbs 0.0201347480573\n",
      "chain 0.0146024182525\n",
      "metropolis 0.0141318353237\n",
      "sample 0.0121684885417\n",
      "sampler 0.0113809877221\n",
      "Topic 20\n",
      "discriminant 0.0730408933014\n",
      "revenue 0.0313741649626\n",
      "biomass 0.0206910063354\n",
      "allele 0.0139925278996\n",
      "canonical 0.0118995398652\n",
      "snps 0.0106396732782\n",
      "analysis 0.00753534636907\n",
      "alleles 0.00749361292555\n",
      "discriminants 0.0073729788663\n",
      "flies 0.00683801311279\n",
      "Topic 21\n",
      "positive 0.0496425340446\n",
      "false 0.0378856561023\n",
      "precision 0.0371020856442\n",
      "negative 0.0288479888859\n",
      "recall 0.0250982547041\n",
      "genes 0.0205479322801\n",
      "positives 0.0195757854117\n",
      "gene 0.0185365804982\n",
      "curve 0.0183488572328\n",
      "true 0.0168709384781\n",
      "Topic 22\n",
      "male 0.0296831138689\n",
      "female 0.0271897415805\n",
      "children 0.022484910121\n",
      "birth 0.0219157784689\n",
      "child 0.0209475271632\n",
      "tickets 0.020087900525\n",
      "random 0.015150144652\n",
      "ticket 0.0122408032466\n",
      "number 0.0111247183506\n",
      "boys 0.00873005532896\n",
      "Topic 23\n",
      "protein 0.0391190154317\n",
      "digits 0.0321401779516\n",
      "frac 0.022692015971\n",
      "digit 0.0207368502527\n",
      "sector 0.0140788328361\n",
      "proteins 0.0136724801229\n",
      "server 0.0110139823003\n",
      "boxes 0.00934955954979\n",
      "amino 0.00922738608575\n",
      "sectors 0.00777450401028\n",
      "Topic 24\n",
      "cards 0.0464450280438\n",
      "card 0.0436623833491\n",
      "eigenvectors 0.0302446065273\n",
      "eigenvalues 0.0250776233271\n",
      "eigenvector 0.0116761573252\n",
      "grades 0.0102256565183\n",
      "deck 0.00997918419932\n",
      "grade 0.00831919439529\n",
      "number 0.00800804280947\n",
      "worker 0.00705312072874\n",
      "Topic 25\n",
      "effect 0.0183822667083\n",
      "test 0.0162870130248\n",
      "effects 0.0149337109373\n",
      "group 0.0146737306388\n",
      "data 0.0136122131131\n",
      "model 0.0133457267502\n",
      "random 0.0111144024788\n",
      "groups 0.0105238647189\n",
      "would 0.0104646489957\n",
      "anova 0.0103956883586\n",
      "Topic 26\n",
      "learning 0.0180485040274\n",
      "network 0.0179879226289\n",
      "neural 0.0158407456172\n",
      "weights 0.0107502965163\n",
      "input 0.0103222140976\n",
      "output 0.00984341903104\n",
      "gradient 0.00908007331668\n",
      "networks 0.00887372989275\n",
      "layer 0.00799351968517\n",
      "training 0.00745139103324\n",
      "Topic 27\n",
      "data 0.0138730911139\n",
      "would 0.00935053349667\n",
      "statistics 0.00852344693973\n",
      "statistical 0.0080650753123\n",
      "analysis 0.00682573443284\n",
      "missing 0.00665112516269\n",
      "people 0.00642606084201\n",
      "also 0.00530284876838\n",
      "question 0.00492406985426\n",
      "like 0.00456894944292\n",
      "Topic 28\n",
      "data 0.0228504491005\n",
      "model 0.0184350804589\n",
      "training 0.0124699463797\n",
      "test 0.012231106629\n",
      "validation 0.0113780926728\n",
      "class 0.0106279605245\n",
      "cross 0.0104023846651\n",
      "using 0.0102593703155\n",
      "classification 0.00839156276286\n",
      "features 0.00742314168971\n",
      "Topic 29\n",
      "harmonic 0.0246598044753\n",
      "jackknife 0.0180951571841\n",
      "expenses 0.0121975392921\n",
      "disturbance 0.0115810033541\n",
      "focal 0.00930928336499\n",
      "delete 0.00714640050924\n",
      "resistance 0.00683515633901\n",
      "disturbances 0.00603172048108\n",
      "equivariance 0.00580391323411\n",
      "cage 0.00561823416847\n"
     ]
    }
   ],
   "source": [
    "# Display topics with top 10 words + its probabilities\n",
    "for i in range(len(termIndices)):\n",
    "    print \"Topic %d\" % ( i )\n",
    "    for j in range(10):\n",
    "        print masterWords[termIndices[i][j]], termWeights[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of topics\n",
    "Many of the topics seems to be related to machine learning and statistics. For example, topic 6 above has words such as distribution, function, matrix, random, variance, likelihood, mean and variables. There are also some other topics that are related to text mining such as topic 14, which has words such as word, words, text, sequence, and document. Some other topic includes a topic about sampling (topic 19), which has words such as sampling, species, mcmc, monte, carlo, and gibbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents with its topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(topicDistribution=DenseVector([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.3693, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.3829, 0.0001, 0.0123, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.2329, 0.0001, 0.0001]))\n"
     ]
    }
   ],
   "source": [
    "docTransformed = model.transform(parsedDFTest)\n",
    "documentIds = docTransformed.select('id').collect()\n",
    "documentFeatures = docTransformed.select('features').collect()\n",
    "documentTopicDistribtions = docTransformed.select('topicDistribution').collect()\n",
    "\n",
    "print documentTopicDistribtions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack_data_docs = stack_data.map(lambda x: (x.split(',')[0], x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247361,\"Normality test before testing the difference between two groups. Is it necessary? I would like to know, when comparing the means in two groups, one with 15 patients and another with 70, if it is necessary to test for normality. While it is possible to test for normality, it is often not very useful to do so. Very few datasets come from an exactly normal distribution and many parametric statistical procedures work well even when the distribution is only \"\"kind of normalish\"\".   (I will note that the unequal sample size may mean that procedures might not be quite so robust to departures from normality as would be the case with equal samples.)  When the sample is small it contains little information about its underlying distribution and so the normal distribution test has low power and you get lots of false negatives. Conversely, when the sample is large and the test has high power, it starts to indicate significant departures in cases where the distribution is close enough to normal that there is no real problem.  Examine your data in a couple of normal distribution plots to get a feel for the shape of the distributions. If there is substantial deviation then you can either transform the data (log transformations are often appropriate) or use non-parametric methods. With sample sizes of 17 and 70 most non-parametric tests will have good power relative to the normal distribution based tests. For example, a permutations test will power equal to that of a Student's t-test.  Really you should provide a lot more information in your question, such as what the measurements are, what sort of tests you wish to perform, whether the research is exploratory or designed, what hypotheses you are interested in, and so on. That way the answers can be more specific and you will gain more assistance.\"\n",
      "\n",
      "Most Probable Topic\n",
      "test\n",
      "sample\n",
      "distribution\n",
      "mean\n",
      "value\n",
      "standard\n",
      "data\n",
      "confidence\n",
      "would\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "docNumber = 8999\n",
    "docID = str(documentIds[docNumber]).split('=')[1][:-1]\n",
    "docTopics = [ float(i) for i in str(documentTopicDistribtions[docNumber]).split('[')[1][:-3].split(',')]\n",
    "index, value = max(enumerate(docTopics), key=operator.itemgetter(1))\n",
    "for i in stack_data_docs:\n",
    "    if int(\"{:.0f}\".format(float(i[0]))) == int(docID):\n",
    "        print i[1]\n",
    "        print 'Most Probable Topic'\n",
    "        for j in range(10):\n",
    "            print masterWords[termIndices[index][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69878,\"Identifying potential for missingness in a seasonal time series to bias period-averages I have high-frequency time series (observations every few minutes) for which I wish to compute daily averages. The data exhibit a strong diel cycle. Sometimes observations are missing in the time series.    For example, I could have 50% of the data missing on a given day.  The effect of this missingness on the daily average isn't immediately obvious given the proportion of missing data. Evenly spaced missingness has little affect (say that 5-minute and 10-minute data give ~same daily average), whereas if all of the missing data are concentrated around the daily minimum/maximum then the daily mean would be biased.  However, the effect of missingness on the daily average not only dependent on the percent of data missing, but also on the evenness of that missingness.  Does anyone know of a good approach for characterizing the potential for missingness to bias the daily average? I'm envisioning some sort of index that incorporates the extent and temporal distribution of missing values, and other factors I may not be considering. Thanks for any guidance.  The figure and R code below use a simple sine wave to illustrate my thinking:     \"\n",
      "\n",
      "Most Probable Topic\n",
      "time\n",
      "series\n",
      "data\n",
      "model\n",
      "would\n",
      "year\n",
      "using\n",
      "forecast\n",
      "arima\n",
      "models\n"
     ]
    }
   ],
   "source": [
    "docNumber = 2379\n",
    "docID = str(documentIds[docNumber]).split('=')[1][:-1]\n",
    "docTopics = [ float(i) for i in str(documentTopicDistribtions[docNumber]).split('[')[1][:-3].split(',')]\n",
    "index, value = max(enumerate(docTopics), key=operator.itemgetter(1))\n",
    "for i in stack_data_docs:\n",
    "    if int(\"{:.0f}\".format(float(i[0]))) == int(docID):\n",
    "        print i[1]\n",
    "        print 'Most Probable Topic'\n",
    "        for j in range(10):\n",
    "            print masterWords[termIndices[index][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38380,\"Consequences from comparison of two models I have time series data for two firms relating cost to quantity produced and I model the two firms' cost functions separately using OLS:  Firm 1:cost_1t= ß_1+ß_2 qty_1t+ ß_3 qty_1t^2+ ß_4 qty_1t^3+ e_1t Firm 2:cost_2t= ß_1+ß_2 qty_2t+ ß_3 qty_2t^2+ ß_4 qty_2t^3+ e_2t  It is believed the firms have the same regression coefficients but doubted the error variances are the same (and a Goldfeld-Quandt test shows the latter).  What are the consequences for the coefficient estimates derived above if:   (i) the 2 firms actually have different variances but the same coefficients and; (ii) the 2 firms have different coefficients but the same variance?  Note, the two separate regressions for the firms appear to show different coefficients.\"\n",
      "\n",
      "Most Probable Topic\n",
      "model\n",
      "regression\n",
      "variables\n",
      "variable\n",
      "data\n",
      "linear\n",
      "would\n",
      "values\n",
      "using\n",
      "models\n"
     ]
    }
   ],
   "source": [
    "docNumber = 1237\n",
    "docID = str(documentIds[docNumber]).split('=')[1][:-1]\n",
    "docTopics = [ float(i) for i in str(documentTopicDistribtions[docNumber]).split('[')[1][:-3].split(',')]\n",
    "index, value = max(enumerate(docTopics), key=operator.itemgetter(1))\n",
    "for i in stack_data_docs:\n",
    "    if int(\"{:.0f}\".format(float(i[0]))) == int(docID):\n",
    "        print i[1]\n",
    "        print 'Most Probable Topic'\n",
    "        for j in range(10):\n",
    "            print masterWords[termIndices[index][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment \n",
    "1. The assigned topics seem to make sense. For document 247361, the most probable topic includes words such as test, sample and distribution. The document seems to be about doing a normality test between two groups, and the topic is about testing and statistics. \n",
    "\n",
    "2. For document 69878, the major topic is about time, series, data and model. Reading the document, it seems like there's some sort of time-series data involved and this person is trying to model it. So, the assigned most probable topic makes sense.\n",
    "\n",
    "3. For document 38380, the major topic of this document is about regression and modelling, which is what the most probable topic indicates.\n",
    "\n",
    "Overall, it's quite remarkable how well the topics describe the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2(c): perplexity calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Perplexity(\\theta) = \\bigg( \\prod_{D}p_\\theta (D) \\bigg)^{-1/\\sum_D|D|}$$\n",
    "Where D is a test document with |D| words. $p_\\theta(D)$ for a test document that we have not seen before. However, we have already trained the model on training set, which allows us to have the topic-distributions already. Using Bayesian framework + chain rule, we can see that $p_\\theta(D)$ can be expressed as the geometric mean of conditional probabilities of the words that we have already trained with. So, $\\prod_{D}p_\\theta (D) = \\prod p_\\theta(w_1,...,w_N) = \\prod_{n=1}^N p_\\theta (w_n|w_1,...,w_{n-1})$. Also, for set of test documents $\\sum_D |D| = N$. Therefore, leading to \n",
    "$$Perplexity(\\theta) = \\bigg( \\prod_{n=1}^N p_\\theta (w_n|w_1,...,w_{n-1}) \\bigg)^{-1/N}$$\n",
    "\n",
    "To evaluate the test set perplexity for LDA, we need to approximate the topic distribution for the test set and compare that with how well our corpus topic distribution describes it. According to spark.ml library, variational approximation can be used to estimate perplexity. This will return an approximate topic-distribution which will be subtracted from the corpus topic-distribution to get a perplexity value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2(d): perplexity vs. # of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate perplexity\n",
    "parsedDataTest = stack_dataTest.map(lambda x: (clean_string(x)))\\\n",
    "                               .map(lambda x: (int(\"{:.0f}\".format(float(x[0]))), Counter(x[1])))\\\n",
    "                               .map(lambda x: map_wordid_count(x, vocabulary))\\\n",
    "                               .map(list)\n",
    "parsedDFTest = parsedDataTest.toDF([\"id\",\"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0, k= 10\n",
      "training 0.0109116400567\n",
      "learning 0.0106893671543\n",
      "class 0.0105461544937\n",
      "classification 0.00855080847201\n",
      "features 0.00822501126018\n",
      "data 0.00809901597212\n",
      "algorithm 0.00796552253994\n",
      "network 0.00766587456586\n",
      "using 0.00719170410763\n",
      "neural 0.00679148168631\n",
      "Topic 0, k= 20\n",
      "dice 0.0194538260216\n",
      "city 0.0190371539611\n",
      "votes 0.0158420251669\n",
      "vote 0.0132835121217\n",
      "cities 0.012861267694\n",
      "people 0.012179001975\n",
      "brand 0.0118553659989\n",
      "chance 0.011657323141\n",
      "roll 0.0115471337257\n",
      "paradox 0.0102769312937\n",
      "Topic 0, k= 30\n",
      "city 0.041323002283\n",
      "brand 0.0303099942616\n",
      "cities 0.0300035231623\n",
      "paradox 0.0216068469713\n",
      "segment 0.0203014037568\n",
      "share 0.0197146194623\n",
      "segments 0.0135405325888\n",
      "votes 0.013170312332\n",
      "birthday 0.0124029260362\n",
      "market 0.011929767741\n",
      "Topic 0, k= 40\n",
      "brand 0.0431280970002\n",
      "city 0.0405705104515\n",
      "cities 0.0357167260604\n",
      "segment 0.0296356168563\n",
      "share 0.0215518553822\n",
      "segments 0.0186670882336\n",
      "urban 0.0147649137516\n",
      "brands 0.0144783963468\n",
      "rural 0.010973510972\n",
      "market 0.00999139170536\n",
      "Topic 0, k= 50\n",
      "brand 0.0490995968629\n",
      "city 0.0467637654354\n",
      "cities 0.0392563847005\n",
      "share 0.0281633053995\n",
      "market 0.0244119017783\n",
      "segment 0.0222160340793\n",
      "brands 0.0172940385257\n",
      "segments 0.0165424446749\n",
      "urban 0.0151496009493\n",
      "percentage 0.0140534130953\n",
      "Topic 0, k= 60\n",
      "brand 0.0590723279624\n",
      "segment 0.0361458044189\n",
      "share 0.0299238674106\n",
      "segments 0.0265458646012\n",
      "brands 0.0213643198337\n",
      "urban 0.017941612626\n",
      "decline 0.014683910195\n",
      "market 0.014419350129\n",
      "rural 0.0138887445281\n",
      "cities 0.00960841417104\n",
      "Topic 0, k= 70\n",
      "brand 0.0594409454385\n",
      "market 0.0455144154908\n",
      "segment 0.0378102526819\n",
      "share 0.0359718666846\n",
      "segments 0.0224869231358\n",
      "brands 0.0210469495764\n",
      "urban 0.0194008405272\n",
      "decline 0.0155275088258\n",
      "rural 0.0145260193339\n",
      "kidney 0.0062898210503\n",
      "Topic 0, k= 80\n",
      "brand 0.0610400966817\n",
      "cities 0.0419170445308\n",
      "segment 0.0390849448365\n",
      "segments 0.0251148567307\n",
      "brands 0.0222999499242\n",
      "urban 0.0199138015749\n",
      "decline 0.0167982958501\n",
      "share 0.0163925925952\n",
      "market 0.0156625113271\n",
      "rural 0.0141469399381\n",
      "Topic 0, k= 90\n",
      "brand 0.0630472878804\n",
      "segment 0.039888898789\n",
      "market 0.0360241002192\n",
      "cities 0.0339796836282\n",
      "share 0.033137636076\n",
      "segments 0.0266327519841\n",
      "brands 0.0226253555574\n",
      "urban 0.0186320789283\n",
      "decline 0.0166107828823\n",
      "rural 0.0141462922498\n",
      "Topic 0, k= 100\n",
      "brand 0.0650128033465\n",
      "segment 0.0470489039388\n",
      "cities 0.0337022963517\n",
      "segments 0.0324247723078\n",
      "share 0.0245378346954\n",
      "brands 0.0230870439907\n",
      "market 0.0186503694818\n",
      "urban 0.0179840716204\n",
      "decline 0.0176108979196\n",
      "rural 0.0137290363129\n",
      "Topic 0, k= 110\n",
      "brand 0.0659878411551\n",
      "segment 0.0503715239583\n",
      "segments 0.0356287279857\n",
      "share 0.0337482260544\n",
      "cities 0.0255888647813\n",
      "brands 0.0216742528521\n",
      "decline 0.0208414655161\n",
      "market 0.0112772291919\n",
      "kidney 0.00572308958581\n",
      "speeds 0.0032409673191\n",
      "Topic 0, k= 120\n",
      "brand 0.0741092021577\n",
      "share 0.0296835753562\n",
      "cities 0.0284818677238\n",
      "brands 0.0268166896976\n",
      "segment 0.0240988908478\n",
      "decline 0.0199929220661\n",
      "market 0.0189532671162\n",
      "segments 0.0124543385245\n",
      "kidney 0.00768853575988\n",
      "ownership 0.00440602184854\n",
      "Topic 0, k= 130\n",
      "brand 0.079114939103\n",
      "segment 0.0516528727104\n",
      "segments 0.0342884593501\n",
      "brands 0.0286069997522\n",
      "share 0.0257120207022\n",
      "market 0.01350967922\n",
      "wine 0.00426082938764\n",
      "consumers 0.00400569616547\n",
      "clothing 0.0034039634691\n",
      "chord 0.0032120079607\n",
      "Topic 0, k= 140\n",
      "segment 0.0562771403948\n",
      "segments 0.0414130995616\n",
      "cities 0.0349800861619\n",
      "share 0.0236018987799\n",
      "urban 0.0217278358231\n",
      "mape 0.021654944701\n",
      "rural 0.016528282117\n",
      "market 0.0111998439129\n",
      "brand 0.0040375288616\n",
      "trips 0.00393928643734\n",
      "Topic 0, k= 150\n",
      "brand 0.061823825951\n",
      "segment 0.0472365662259\n",
      "cities 0.0425524652995\n",
      "segments 0.0347017608157\n",
      "share 0.0280411814075\n",
      "brands 0.0238343548307\n",
      "urban 0.0198048857669\n",
      "rural 0.0145255693403\n",
      "rectangle 0.0102540776018\n",
      "decline 0.00870349967845\n",
      "Topic 0, k= 160\n",
      "brand 0.0549102448252\n",
      "market 0.0506935692441\n",
      "cities 0.0393519723421\n",
      "segment 0.0361282182007\n",
      "share 0.0327948259471\n",
      "segments 0.0203706895863\n",
      "brands 0.0192815564834\n",
      "city 0.0175540000945\n",
      "urban 0.0159722050087\n",
      "rectangle 0.0126634318269\n",
      "Topic 0, k= 170\n",
      "cities 0.0575609770867\n",
      "segment 0.0538345098995\n",
      "segments 0.0400477407254\n",
      "share 0.0383873695217\n",
      "city 0.0279662751697\n",
      "urban 0.0211793297636\n",
      "rural 0.0161580397666\n",
      "market 0.0120977565611\n",
      "kidney 0.00726867203154\n",
      "trip 0.00644149023556\n",
      "Topic 0, k= 180\n",
      "brand 0.0846942467118\n",
      "segment 0.0367055451392\n",
      "share 0.0354523002435\n",
      "brands 0.0309394154401\n",
      "market 0.0256760569334\n",
      "segments 0.0161517496096\n",
      "urban 0.010661769937\n",
      "rural 0.00871362381579\n",
      "banana 0.00362054387495\n",
      "clothing 0.0034907019178\n",
      "Topic 0, k= 190\n",
      "class 0.000634576164614\n",
      "values 0.000609666004539\n",
      "i've 0.000603828468052\n",
      "question 0.0005754776008\n",
      "formulations 0.000527210810626\n",
      "example 0.000522519043184\n",
      "classifier 0.000514599876084\n",
      "classification 0.000491687342813\n",
      "group 0.00048968561694\n",
      "could 0.000471334102312\n",
      "Topic 0, k= 200\n",
      "share 0.0555954161044\n",
      "segment 0.0378209539637\n",
      "urban 0.0239331987066\n",
      "paradox 0.0237951589679\n",
      "rural 0.0182165453985\n",
      "segments 0.0165185306575\n",
      "simpson's 0.0139985442485\n",
      "market 0.0110806936419\n",
      "kidney 0.00981798592841\n",
      "shares 0.00795764486052\n"
     ]
    }
   ],
   "source": [
    "numTopics = []\n",
    "perplexities = []\n",
    "\n",
    "for i in range(20):\n",
    "    i = (i + 1) * 10\n",
    "    numTopics.append(i)\n",
    "    lda = LDA(k=i, maxIter=50)\n",
    "    model = lda.fit(parsedDF)\n",
    "    perplexity = model.logPerplexity(parsedDFTest)\n",
    "    perplexities.append(perplexity)\n",
    "    termIndices = model.describeTopics().select(\"termIndices\").rdd.map(list).map(lambda x:x[0]).collect()\n",
    "    termWeights = model.describeTopics().select(\"termWeights\").rdd.map(list).map(lambda x:x[0]).collect()\n",
    "    print \"Topic 0, k=\", i\n",
    "    for j in range(10):\n",
    "        print masterWords[termIndices[0][j]], termWeights[0][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200] [7.570712890269092, 7.705686982327903, 7.861924748890604, 8.07195890957693, 8.281346349483734, 8.52087887107468, 8.755343809534672, 9.001391492810448, 9.282977504920812, 9.549741963741807, 9.849841152932228, 10.12630960892443, 10.40568212721225, 10.726585997995038, 11.03235583627134, 11.334030253920009, 11.649509388085194, 11.98946066379855, 12.309946200241583, 12.645161505166872]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f23d3daba50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAETCAYAAAAlCTHcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f23d372a950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print numTopics, perplexities\n",
    "plt.plot(numTopics, perplexities,'-o')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('k (number of topics)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Perplexity', fontsize=16, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "Based on the plot above and the quality of one of the topics, k=10 seems to be the best model. LogPerplexity is the lowest for k=10, and looking at the topic words, it seems to be populated with words that are related to machine learning such as training, learning, data, class, classification and algorithm. However, as k increases, the topic words begin to make less and less sense. Even at k=20, there's a combination of dice+chances, cities and votes, I'm not quite sure what this topic is suppsoed to be...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
